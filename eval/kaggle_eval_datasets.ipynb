{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5017a5a",
   "metadata": {},
   "source": [
    "### 1. Kaggle Evaluation Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd440d18",
   "metadata": {},
   "source": [
    "#### 1.1 Unzip datasets in each folder\n",
    "- We downloaded datasets using Kaggle's API, resulting in 5,221 folders. Each folder contains a `dataset-metadata.json` file with metadata in the following structure:\n",
    "  ```\n",
    "{\n",
    "  \"id\":\n",
    "  \"id_no\":\n",
    "  \"datasetSlugNullable\":\n",
    "  \"ownerUserNullable\":\n",
    "  \"usabilityRatingNullable\":\n",
    "  \"titleNullable\":\n",
    "  \"subtitleNullable\":\n",
    "  \"descriptionNullable\":\n",
    "  \"datasetId\":\n",
    "  \"datasetSlug\":\n",
    "  \"hasDatasetSlug\":\n",
    "  \"ownerUser\":\n",
    "  \"hasOwnerUser\":\n",
    "  \"usabilityRating\":\n",
    "  \"hasUsabilityRating\":\n",
    "  \"totalViews\":\n",
    "  \"totalVotes\":\n",
    "  \"totalDownloads\":\n",
    "  \"title\":\n",
    "  \"hasTitle\":\n",
    "  \"subtitle\":\n",
    "  \"hasSubtitle\":\n",
    "  \"description\":\n",
    "  \"hasDescription\":\n",
    "  \"isPrivate\":\n",
    "  \"keywords\":\n",
    "  \"licenses\":\n",
    "  \"collaborators\":\n",
    "  \"data\":\n",
    "}\n",
    "```\n",
    "- Each folder also contains a zip file with the corresponding datasets. Our first step is to iterate through all 5,221 folders and unzip all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba19156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the folders with zip files\n",
    "base_directory = os.path.join(os.getcwd(), \"kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over all folders in the base directory\n",
    "# for folder_name in os.listdir(base_directory):\n",
    "#     folder_path = os.path.join(base_directory, folder_name)\n",
    "    \n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Look for zip files in the directory\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             if file_name.endswith(\".zip\"):\n",
    "#                 zip_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "#                 # Try to unzip the file\n",
    "#                 try:\n",
    "#                     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#                         zip_ref.extractall(folder_path)\n",
    "#                     print(f\"Unzipped: {zip_path}\")\n",
    "#                 except zipfile.BadZipFile as e:\n",
    "#                     print(f\"Failed to unzip {zip_path}: {e}\")\n",
    "\n",
    "# print(\"All zip files have been unzipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a43d4",
   "metadata": {},
   "source": [
    "#### 1.2 Check the files under each folder downloaded from Kaggle\n",
    "\n",
    "- Original unfiltered # of files in total: 12,533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42db66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71608f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "folder_names = []  # list of names of folders containing each dataset\n",
    "dataset_names = []  # list of \"title\" fields from dataset-metadata.json\n",
    "file_names = []  # list of all files in each folder except for dataset-metadata.json and zip files\n",
    "licenses = []  # list of \"licenses name\" fields from dataset-metadata.json\n",
    "descriptions = []  # list of \"description\" fields from dataset-metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b939835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each Kaggle dataset folder\n",
    "def process_dataset_folder(folder_path):\n",
    "    try:\n",
    "        # Path to the dataset-metadata.json file\n",
    "        metadata_path = os.path.join(folder_path, 'dataset-metadata.json')\n",
    "        \n",
    "        # Read the dataset-metadata.json file\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Extract the required information\n",
    "        dataset_name = metadata.get('title', '')\n",
    "        license_name = metadata.get('licenses', [{}])[0].get('name', '')\n",
    "        description = metadata.get('description', '')\n",
    "\n",
    "        # List all files in the folder except for dataset-metadata.json and zip files\n",
    "        files = [f for f in os.listdir(folder_path) if f != 'dataset-metadata.json' and not f.endswith('.zip')]\n",
    "\n",
    "        # Store the information in the lists\n",
    "        for file in files:\n",
    "            folder_names.append(os.path.basename(folder_path))\n",
    "            dataset_names.append(dataset_name)\n",
    "            file_names.append(file)\n",
    "            licenses.append(license_name)\n",
    "            descriptions.append(description)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each folder in the base directory\n",
    "for folder_name in tqdm(os.listdir(base_directory), desc=\"Processing Kaggle Datasets\"):\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_dataset_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579800a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the DataFrame\n",
    "data = {\n",
    "    'Folder Name': folder_names,\n",
    "    'Dataset Name': dataset_names,\n",
    "    'File Name': file_names,\n",
    "    'License': licenses,\n",
    "    'Description': descriptions\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adjust display options to show the complete DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168749e",
   "metadata": {},
   "source": [
    "#### 1.3 Datasets pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de169c",
   "metadata": {},
   "source": [
    "##### 1.3.1 Filter out csv files\n",
    "- 12,533 -> 8,629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract file types (extensions) and make them lowercase\n",
    "df['File Type'] = df['File Name'].apply(lambda x: os.path.splitext(x)[1].lower())\n",
    "\n",
    "# Analyze the file types\n",
    "file_type_counts = df['File Type'].value_counts().reset_index()\n",
    "file_type_counts.columns = ['File Type', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original df to include only CSV files\n",
    "csv_df = df[df['File Type'] == '.csv']\n",
    "\n",
    "# Reset the index of the filtered DataFrame\n",
    "csv_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b12e8",
   "metadata": {},
   "source": [
    "##### 1.3.2 Filter out datasets w/ allowed licenses\n",
    "- 8,629 -> 7,012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the license\n",
    "license_counts = csv_df['License'].value_counts().reset_index()\n",
    "license_counts.columns = ['License', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44642edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the csv_df for allowed licenses\n",
    "not_allowed_licenses = [\n",
    "    \"unknown\",\n",
    "    \"copyright-authors\",\n",
    "]\n",
    "\n",
    "csv_df_licensed = csv_df[~csv_df['License'].isin(not_allowed_licenses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b739b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df_licensed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac70dee",
   "metadata": {},
   "source": [
    "##### 1.3.3 Filter out datasets w/ description\n",
    "- 7,012 -> 6,520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out datasets with descriptions\n",
    "csv_df_desc = csv_df_licensed[csv_df_licensed['Description'].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c151ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cacfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(csv_df_desc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af684d",
   "metadata": {},
   "source": [
    "##### 1.3.4 Check dataset-table mapping\n",
    "- 6,520 -> 2,357 (one-to-one mapping only: each dataset contains ONLY one table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Dataset Name and count the number of File Names for each Dataset\n",
    "table_count_per_dataset = csv_df_desc.groupby('Dataset Name').size().reset_index(name='Table Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_count_per_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets that have only one table\n",
    "one_table_per_dataset = table_count_per_dataset[table_count_per_dataset['Table Count'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef53462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(one_table_per_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with the original dataframe `csv_df_desc` to get all other attributes\n",
    "one_table_datasets = pd.merge(one_table_per_dataset, csv_df_desc, on='Dataset Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(one_table_datasets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfa98a",
   "metadata": {},
   "source": [
    "- 6,520 -> 4,163 (one-to-multiple mapping: each dataset contains MULTIPLE tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets that have multiple tables\n",
    "multi_table_per_dataset = table_count_per_dataset[table_count_per_dataset['Table Count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3305bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_per_dataset['Table Count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with the original dataframe `csv_df_desc` to get all other attributes\n",
    "multi_table_datasets = pd.merge(multi_table_per_dataset, csv_df_desc, on='Dataset Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_table_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4926a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chardet\n",
    "\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(10000)  # Read only the first 10k bytes\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_multiple_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252']\n",
    "    detected_encoding = detect_encoding(file_path)\n",
    "    encodings.insert(0, detected_encoding)  # Try detected encoding first\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(file_path, nrows=0, encoding=encoding)  # Read only the header\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return None  # If all attempts fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema_consistency(folder_name, base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if not csv_files:\n",
    "        return False\n",
    "\n",
    "    schemas = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            df = read_csv_with_multiple_encodings(file_path)\n",
    "            if df is not None:\n",
    "                schemas.append(set(df.columns))\n",
    "            else:\n",
    "                print(f\"Error reading {file_path}: Unable to decode with common encodings\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Check if all schemas are identical\n",
    "    first_schema = schemas[0]\n",
    "    for schema in schemas:\n",
    "        if schema != first_schema:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9acb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column \"Schema Consistency\" to `multi_table_datasets`\n",
    "schema_consistency = []\n",
    "\n",
    "for folder_name in tqdm(multi_table_datasets['Folder Name'].unique(), desc=\"Checking Schema Consistency\"):\n",
    "    consistency = check_schema_consistency(folder_name, base_directory)\n",
    "    schema_consistency.append((folder_name, consistency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the schema_consistency list to a DataFrame\n",
    "schema_consistency_df = pd.DataFrame(schema_consistency, columns=['Folder Name', 'Schema Consistency'])\n",
    "\n",
    "# Join the schema consistency results with `multi_table_datasets`\n",
    "multi_table_datasets = pd.merge(multi_table_datasets, schema_consistency_df, on='Folder Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_datasets['Schema Consistency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_same_schema = multi_table_datasets[multi_table_datasets['Schema Consistency'] == True]\n",
    "multi_table_diff_schema = multi_table_datasets[multi_table_datasets['Schema Consistency'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4993ec",
   "metadata": {},
   "source": [
    "#### 1.4 Visualize # of datasets after different stages of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the stacked bar chart\n",
    "categories = ['One-to-One Mapping', 'One-to-Multiple Mapping']\n",
    "values_one_to_one = [2357, 0]\n",
    "values_multiple_consistent = [0, 1461]\n",
    "values_multiple_inconsistent = [0, 2702]\n",
    "\n",
    "# Data for the funnel chart\n",
    "stages = [\n",
    "    \"Total Datasets from Kaggle\",  # df\n",
    "    \"Filter by CSV\",  # csv_df\n",
    "    \"Filter by Licensing\",  # csv_df_licensed\n",
    "    \"Filter by Descriptions\",  # csv_df_desc\n",
    "]\n",
    "counts = [12533, 8629, 7012, 6520]\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plotting the funnel chart on the first subplot\n",
    "ax1.barh(stages, counts)\n",
    "\n",
    "# Adding labels on the bars for the funnel chart\n",
    "for index, value in enumerate(counts):\n",
    "    ax1.text(value, index, str(value), va='center')\n",
    "\n",
    "# Setting title and labels for the first plot\n",
    "ax1.set_title('Funnel Chart of Dataset Filtering Stages')\n",
    "ax1.set_xlabel('Number of Files')\n",
    "ax1.set_ylabel('Filtering Stages')\n",
    "ax1.invert_yaxis()  # Reverse the order of stages for a funnel effect\n",
    "\n",
    "# Plotting the stacked bar chart on the second subplot\n",
    "p1 = ax2.bar(categories, values_one_to_one, label='One-to-One Mapping')\n",
    "p2 = ax2.bar(categories, values_multiple_consistent, bottom=values_one_to_one, label='One-to-Multiple (Consistent Schema)')\n",
    "p3 = ax2.bar(categories, values_multiple_inconsistent, bottom=[i+j for i,j in zip(values_one_to_one, values_multiple_consistent)], label='One-to-Multiple (Inconsistent Schema)')\n",
    "\n",
    "# Add text labels on the bars for the stacked bar chart\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        if yval > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_y() + yval/2, int(yval), ha='center', va='center', color='white')\n",
    "\n",
    "add_labels(p1, ax2)\n",
    "add_labels(p2, ax2)\n",
    "add_labels(p3, ax2)\n",
    "\n",
    "# Setting title and labels for the second plot\n",
    "ax2.set_title('Dataset-Table Mapping')\n",
    "ax2.set_xlabel('Mapping Type')\n",
    "ax2.set_ylabel('Number of Tables')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb330b",
   "metadata": {},
   "source": [
    "#### 1.5 Generate keywords, tasks, & other needed metadata for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to CSV files\n",
    "one_table_datasets.to_csv('one_table_datasets.csv', index=False)\n",
    "multi_table_same_schema.to_csv('multi_table_same_schema.csv', index=False)\n",
    "multi_table_diff_schema.to_csv('multi_table_diff_schema.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619adbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "    one_table_datasets header: {one_table_datasets.columns.tolist()}\n",
    "    multi_table_same_schema header: {multi_table_same_schema.columns.tolist()}\n",
    "    multi_table_diff_schema header: {multi_table_diff_schema.columns.tolist()}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8420c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5363125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into dataframe\n",
    "one_table_datasets = pd.read_csv('raw_kaggle_data/one_table_datasets.csv')\n",
    "multi_table_same_schema = pd.read_csv('raw_kaggle_data/multi_table_same_schema.csv')\n",
    "multi_table_diff_schema = pd.read_csv('raw_kaggle_data/multi_table_diff_schema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d341d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61ed846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key and model name\n",
    "load_dotenv()\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a0358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords_and_queries(description, dataset_name, filenames=None):\n",
    "    if filenames:\n",
    "        filenames_str = \"\\n- \".join(filenames)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            Given a dataset that contains CSV files with the below file names:\n",
    "                - {filenames_str}\n",
    "            And the description of the dataset provided below:\n",
    "                {description}\n",
    "\n",
    "            Generate a dictionary in JSON format with the CSV file names as keys and as values a list of 4 semantically distinct data content keywords that describe the expected content of each CSV file but do not describe specific analytic tasks possible with the file.\n",
    "            For each CSV file, also add a list of 3 semantically distinct analytics task sentences that can be performed with the described CSV file, e.g. develop ML model to predict XYZ.\n",
    "\n",
    "            Example response format:\n",
    "            {{\n",
    "                \"csv file 1\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }},\n",
    "                \"csv file 2\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "            Based on the dataset description provided below, generate a dictionary in JSON format with the dataset name as key and as values a list of 4 semantically distinct data content keywords that describe the expected content of the dataset but do not describe specific analytic tasks possible with the data. \n",
    "            Also, add a list of 3 semantically distinct analytics task sentences that can be performed with the described dataset, e.g. develop ML model to predict XYZ.\n",
    "            If it is hard to complete the task, return an empty dictionary instead.\n",
    "\n",
    "            Dataset Description:\n",
    "            \"{description}\"\n",
    "\n",
    "            Example response format:\n",
    "            {{\n",
    "                \"{dataset_name}\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed generate keywords and task-based queries for tables.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    keywords_and_queries = response.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(keywords_and_queries)\n",
    "        token_usage = response.usage\n",
    "        return data, token_usage\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to decode JSON response\"}, response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95569038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets(one_table_datasets, multi_table_same_schema, multi_table_diff_schema):\n",
    "    results = []\n",
    "    token_usages = []\n",
    "\n",
    "    # Process one_table_datasets and multi_table_same_schema\n",
    "    for df, table_type in zip([one_table_datasets, multi_table_same_schema], ['one_table', 'multi_table_same_schema']):\n",
    "        for dataset_name, group in tqdm(df.groupby('Dataset Name'), desc=f\"Processing {table_type} datasets\"):\n",
    "            description = group.iloc[0]['Description']\n",
    "            keywords_and_queries, token_usage = generate_keywords_and_queries(description, dataset_name)\n",
    "            token_usages.append(token_usage)\n",
    "            for _, row in group.iterrows():\n",
    "                file_name = row['File Name']\n",
    "                keywords = keywords_and_queries.get(dataset_name, {}).get('keywords', [])\n",
    "                task_queries = keywords_and_queries.get(dataset_name, {}).get('task_queries', [])\n",
    "                results.append((row['Dataset Name'], file_name, keywords, task_queries, table_type))\n",
    "    \n",
    "    # Process multi_table_diff_schema\n",
    "    for index, row in tqdm(multi_table_diff_schema.iterrows(), desc=\"Processing multi_table_diff_schema datasets\", total=multi_table_diff_schema.shape[0]):\n",
    "        dataset_name = row['Dataset Name']\n",
    "        description = row['Description']\n",
    "        csv_file = row['File Name']\n",
    "        \n",
    "        keywords_and_queries, token_usage = generate_keywords_and_queries(description, dataset_name, [csv_file])\n",
    "        token_usages.append(token_usage)\n",
    "        keywords = keywords_and_queries.get(csv_file, {}).get('keywords', [])\n",
    "        task_queries = keywords_and_queries.get(csv_file, {}).get('task_queries', [])\n",
    "        results.append((dataset_name, csv_file, keywords, task_queries, 'multi_table_diff_schema'))\n",
    "\n",
    "    return results, token_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f10d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_samples(one_table_datasets, multi_table_same_schema, multi_table_diff_schema, n=5):\n",
    "    one_table_sample = one_table_datasets.sample(n=min(n, len(one_table_datasets)))\n",
    "    multi_table_same_schema_sample = multi_table_same_schema.sample(n=min(n, len(multi_table_same_schema)))\n",
    "    multi_table_diff_schema_sample = multi_table_diff_schema.sample(n=min(n, len(multi_table_diff_schema)))\n",
    "    \n",
    "    return one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f02f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing one_table datasets: 100%|████████████| 20/20 [00:37<00:00,  1.89s/it]\n",
      "Processing multi_table_same_schema datasets: 100%|█| 15/15 [00:23<00:00,  1.54s/\n",
      "Processing multi_table_diff_schema datasets: 100%|█| 20/20 [00:53<00:00,  2.69s/\n"
     ]
    }
   ],
   "source": [
    "# Create 20 test samples\n",
    "one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample = create_test_samples(one_table_datasets, multi_table_same_schema, multi_table_diff_schema, n=20)\n",
    "\n",
    "# Process the test samples\n",
    "test_results, token_usages = process_datasets(one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62d53ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage for request 1: CompletionUsage(completion_tokens=79, prompt_tokens=592, total_tokens=671)\n",
      "Token usage for request 2: CompletionUsage(completion_tokens=84, prompt_tokens=427, total_tokens=511)\n",
      "Token usage for request 3: CompletionUsage(completion_tokens=90, prompt_tokens=954, total_tokens=1044)\n",
      "Token usage for request 4: CompletionUsage(completion_tokens=89, prompt_tokens=430, total_tokens=519)\n",
      "Token usage for request 5: CompletionUsage(completion_tokens=82, prompt_tokens=326, total_tokens=408)\n",
      "Token usage for request 6: CompletionUsage(completion_tokens=95, prompt_tokens=533, total_tokens=628)\n",
      "Token usage for request 7: CompletionUsage(completion_tokens=81, prompt_tokens=307, total_tokens=388)\n",
      "Token usage for request 8: CompletionUsage(completion_tokens=89, prompt_tokens=441, total_tokens=530)\n",
      "Token usage for request 9: CompletionUsage(completion_tokens=86, prompt_tokens=370, total_tokens=456)\n",
      "Token usage for request 10: CompletionUsage(completion_tokens=74, prompt_tokens=421, total_tokens=495)\n",
      "Token usage for request 11: CompletionUsage(completion_tokens=89, prompt_tokens=417, total_tokens=506)\n",
      "Token usage for request 12: CompletionUsage(completion_tokens=85, prompt_tokens=596, total_tokens=681)\n",
      "Token usage for request 13: CompletionUsage(completion_tokens=85, prompt_tokens=566, total_tokens=651)\n",
      "Token usage for request 14: CompletionUsage(completion_tokens=90, prompt_tokens=272, total_tokens=362)\n",
      "Token usage for request 15: CompletionUsage(completion_tokens=80, prompt_tokens=310, total_tokens=390)\n",
      "Token usage for request 16: CompletionUsage(completion_tokens=92, prompt_tokens=421, total_tokens=513)\n",
      "Token usage for request 17: CompletionUsage(completion_tokens=91, prompt_tokens=285, total_tokens=376)\n",
      "Token usage for request 18: CompletionUsage(completion_tokens=80, prompt_tokens=304, total_tokens=384)\n",
      "Token usage for request 19: CompletionUsage(completion_tokens=100, prompt_tokens=355, total_tokens=455)\n",
      "Token usage for request 20: CompletionUsage(completion_tokens=79, prompt_tokens=357, total_tokens=436)\n",
      "Token usage for request 21: CompletionUsage(completion_tokens=81, prompt_tokens=620, total_tokens=701)\n",
      "Token usage for request 22: CompletionUsage(completion_tokens=80, prompt_tokens=391, total_tokens=471)\n",
      "Token usage for request 23: CompletionUsage(completion_tokens=84, prompt_tokens=712, total_tokens=796)\n",
      "Token usage for request 24: CompletionUsage(completion_tokens=90, prompt_tokens=944, total_tokens=1034)\n",
      "Token usage for request 25: CompletionUsage(completion_tokens=77, prompt_tokens=271, total_tokens=348)\n",
      "Token usage for request 26: CompletionUsage(completion_tokens=91, prompt_tokens=1480, total_tokens=1571)\n",
      "Token usage for request 27: CompletionUsage(completion_tokens=90, prompt_tokens=443, total_tokens=533)\n",
      "Token usage for request 28: CompletionUsage(completion_tokens=101, prompt_tokens=453, total_tokens=554)\n",
      "Token usage for request 29: CompletionUsage(completion_tokens=97, prompt_tokens=592, total_tokens=689)\n",
      "Token usage for request 30: CompletionUsage(completion_tokens=88, prompt_tokens=271, total_tokens=359)\n",
      "Token usage for request 31: CompletionUsage(completion_tokens=82, prompt_tokens=320, total_tokens=402)\n",
      "Token usage for request 32: CompletionUsage(completion_tokens=87, prompt_tokens=701, total_tokens=788)\n",
      "Token usage for request 33: CompletionUsage(completion_tokens=75, prompt_tokens=779, total_tokens=854)\n",
      "Token usage for request 34: CompletionUsage(completion_tokens=108, prompt_tokens=428, total_tokens=536)\n",
      "Token usage for request 35: CompletionUsage(completion_tokens=80, prompt_tokens=333, total_tokens=413)\n",
      "Token usage for request 36: CompletionUsage(completion_tokens=95, prompt_tokens=569, total_tokens=664)\n",
      "Token usage for request 37: CompletionUsage(completion_tokens=91, prompt_tokens=320, total_tokens=411)\n",
      "Token usage for request 38: CompletionUsage(completion_tokens=78, prompt_tokens=873, total_tokens=951)\n",
      "Token usage for request 39: CompletionUsage(completion_tokens=85, prompt_tokens=506, total_tokens=591)\n",
      "Token usage for request 40: CompletionUsage(completion_tokens=410, prompt_tokens=784, total_tokens=1194)\n",
      "Token usage for request 41: CompletionUsage(completion_tokens=102, prompt_tokens=527, total_tokens=629)\n",
      "Token usage for request 42: CompletionUsage(completion_tokens=93, prompt_tokens=687, total_tokens=780)\n",
      "Token usage for request 43: CompletionUsage(completion_tokens=100, prompt_tokens=518, total_tokens=618)\n",
      "Token usage for request 44: CompletionUsage(completion_tokens=690, prompt_tokens=847, total_tokens=1537)\n",
      "Token usage for request 45: CompletionUsage(completion_tokens=78, prompt_tokens=396, total_tokens=474)\n",
      "Token usage for request 46: CompletionUsage(completion_tokens=88, prompt_tokens=462, total_tokens=550)\n",
      "Token usage for request 47: CompletionUsage(completion_tokens=100, prompt_tokens=718, total_tokens=818)\n",
      "Token usage for request 48: CompletionUsage(completion_tokens=84, prompt_tokens=973, total_tokens=1057)\n",
      "Token usage for request 49: CompletionUsage(completion_tokens=83, prompt_tokens=452, total_tokens=535)\n",
      "Token usage for request 50: CompletionUsage(completion_tokens=301, prompt_tokens=384, total_tokens=685)\n",
      "Token usage for request 51: CompletionUsage(completion_tokens=94, prompt_tokens=795, total_tokens=889)\n",
      "Token usage for request 52: CompletionUsage(completion_tokens=85, prompt_tokens=857, total_tokens=942)\n",
      "Token usage for request 53: CompletionUsage(completion_tokens=79, prompt_tokens=476, total_tokens=555)\n",
      "Token usage for request 54: CompletionUsage(completion_tokens=95, prompt_tokens=367, total_tokens=462)\n",
      "Token usage for request 55: CompletionUsage(completion_tokens=306, prompt_tokens=397, total_tokens=703)\n"
     ]
    }
   ],
   "source": [
    "# Print token usage information\n",
    "for i, usage in enumerate(token_usages):\n",
    "    print(f\"Token usage for request {i+1}: {usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "test_results_df = pd.DataFrame(test_results, columns=['Dataset Name', 'CSV File', 'Keywords', 'Task Queries', 'Table Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db065cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the `test_results_df` back to the corresponding original DataFrames\n",
    "one_table_sample = pd.merge(one_table_sample, test_results_df[test_results_df['Table Type'] == 'one_table'], \n",
    "                            left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')\n",
    "\n",
    "multi_table_same_schema_sample = pd.merge(multi_table_same_schema_sample, test_results_df[test_results_df['Table Type'] == 'multi_table_same_schema'], \n",
    "                                          left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')\n",
    "\n",
    "multi_table_diff_schema_sample = pd.merge(multi_table_diff_schema_sample, test_results_df[test_results_df['Table Type'] == 'multi_table_diff_schema'], \n",
    "                                          left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the DataFrames\n",
    "union_df_sample = pd.concat([one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f099c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_sample.to_csv('keyword_query_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
